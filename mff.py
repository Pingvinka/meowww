import torch
from transformers import AutoModel, AutoTokenizer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import json
from typing import List, Dict, Tuple
import warnings
warnings.filterwarnings('ignore')

class JinaEmbedder:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Jina Embeddings"""
    
    def __init__(self, model_name="jinaai/jina-embeddings-v2-base-en"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.model.eval()
    
    def get_embeddings(self, texts: List[str]) -> np.ndarray:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
        embeddings = []
        
        for text in texts:
            encoded_input = self.tokenizer(
                text, 
                padding=True, 
                truncation=True, 
                return_tensors='pt',
                max_length=512
            )
            
            with torch.no_grad():
                model_output = self.model(**encoded_input)
                # –ë–µ—Ä–µ–º embedding [CLS] —Ç–æ–∫–µ–Ω–∞
                text_embedding = model_output.last_hidden_state[:, 0, :].cpu().numpy()
                embeddings.append(text_embedding[0])
        
        return np.array(embeddings)

class VectorStore:
    """–ü—Ä–æ—Å—Ç–∞—è –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.documents = []
        self.embeddings = None
        self.metadata = []
    
    def add_documents(self, documents: List[str], metadata: List[Dict] = None):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É"""
        self.documents.extend(documents)
        
        if metadata:
            self.metadata.extend(metadata)
        else:
            self.metadata.extend([{}] * len(documents))
    
    def build_index(self, embedder: JinaEmbedder):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
        print("–°—Ç—Ä–æ–∏–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å...")
        self.embeddings = embedder.get_embeddings(self.documents)
        print(f"–ò–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω. –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self.embeddings.shape}")
    
    def search(self, query_embedding: np.ndarray, top_k: int = 3) -> List[Tuple[str, float, Dict]]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if self.embeddings is None:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–¥–µ–∫—Å!")
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarities = cosine_similarity([query_embedding], self.embeddings)[0]
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-K —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            results.append((
                self.documents[idx],
                similarities[idx],
                self.metadata[idx]
            ))
        
        return results

class RAGSystem:
    """–ü–æ–ª–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞"""
    
    def __init__(self, embedder_model: str = "jinaai/jina-embeddings-v2-base-en"):
        self.embedder = JinaEmbedder(embedder_model)
        self.vector_store = VectorStore()
        self.setup_demo_data()
    
    def setup_demo_data(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        
        # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–µ
        documents = [
            # –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –ò–ò
            "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ –æ–±–ª–∞—Å—Ç—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ —è–≤–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.",
            "–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º —Å–ª–æ–µ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ª–æ–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Ç–∞–∫–∏—Ö –∫–∞–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç.",
            "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã - —ç—Ç–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞.",
            "GPT-4 —è–≤–ª—è–µ—Ç—Å—è –∫—Ä—É–ø–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª—å—é —Å–ø–æ—Å–æ–±–Ω–æ–π –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —á–µ–ª–æ–≤–µ–∫–æ–æ–±—Ä–∞–∑–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏.",
            
            # RAG —Ç–µ—Ö–Ω–∏–∫–∏
            "Retrieval-Augmented Generation (RAG) —Å–æ—á–µ—Ç–∞–µ—Ç –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤.",
            "RAG —Å–∏—Å—Ç–µ–º—ã —Å–Ω–∞—á–∞–ª–∞ –Ω–∞—Ö–æ–¥—è—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π, –∞ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É—é—Ç –∏—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞.",
            "–í–µ–∫—Ç–æ—Ä–Ω—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Ö—Ä–∞–Ω—è—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞.",
            "–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ - —ç—Ç–æ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ.",
            
            # –ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è
            "–ò–ò –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —á–∞—Ç-–±–æ—Ç–∞—Ö, –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞—Ö –∏ —Å–∏—Å—Ç–µ–º–∞—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.",
            "–ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–∞—à–∏–Ω–∞–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –≤–∏–¥–µ–æ.",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP) –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º –ø–æ–Ω–∏–º–∞—Ç—å –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —è–∑—ã–∫.",
            "–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ò–ò –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∏ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏.",
            
            # Jina Embeddings
            "Jina Embeddings - —ç—Ç–æ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —è–∑—ã–∫–æ–≤.",
            "Jina Embeddings v2 –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ 8192 —Ç–æ–∫–µ–Ω–æ–≤ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –¥–ª—è –∑–∞–¥–∞—á –ø–æ–∏—Å–∫–∞.",
            "–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ Jina –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã –≤ RAG —Å–∏—Å—Ç–µ–º–∞—Ö –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞.",
            "MRL —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è –≤ Jina –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ä–∞–∑–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."
        ]
        
        metadata = [
            {"category": "AI —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "source": "demo_data"},
            {"category": "AI —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "source": "demo_data"},
            {"category": "AI –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã", "source": "demo_data"},
            {"category": "AI –º–æ–¥–µ–ª–∏", "source": "demo_data"},
            {"category": "RAG", "source": "demo_data"},
            {"category": "RAG", "source": "demo_data"},
            {"category": "–í–µ–∫—Ç–æ—Ä–Ω—ã–µ –ë–î", "source": "demo_data"},
            {"category": "–≠–º–±–µ–¥–¥–∏–Ω–≥–∏", "source": "demo_data"},
            {"category": "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è", "source": "demo_data"},
            {"category": "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è", "source": "demo_data"},
            {"category": "NLP", "source": "demo_data"},
            {"category": "–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã", "source": "demo_data"},
            {"category": "Jina", "source": "demo_data"},
            {"category": "Jina", "source": "demo_data"},
            {"category": "Jina", "source": "demo_data"},
            {"category": "Jina", "source": "demo_data"}
        ]
        
        self.vector_store.add_documents(documents, metadata)
        self.vector_store.build_index(self.embedder)
        print("‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω—ã!")
    
    def retrieve(self, query: str, top_k: int = 3) -> List[Tuple[str, float, Dict]]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        query_embedding = self.embedder.get_embeddings([query])[0]
        results = self.vector_store.search(query_embedding, top_k)
        return results
    
    def generate_prompt(self, query: str, context_docs: List[Tuple[str, float, Dict]]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è LLM"""
        context_text = "\n\n".join([f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç {i+1} (—Å—Ö–æ–¥—Å—Ç–≤–æ: {score:.3f}): {doc}" 
                                  for i, (doc, score, metadata) in enumerate(context_docs)])
        
        prompt = f"""–ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω—ã–µ –Ω–∏–∂–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, —á—Ç–æ–±—ã —Ç–æ—á–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–ö–û–ù–¢–ï–ö–°–¢:
{context_text}

–í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø: {query}

–ò–ù–°–¢–†–£–ö–¶–ò–ò:
- –û—Ç–≤–µ—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞, —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º
- –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–µ–Ω –∏ –∏—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- –°–æ—Ö—Ä–∞–Ω—è–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–æ–Ω

–û–¢–í–ï–¢:"""
        return prompt
    
    def mock_llm_generate(self, prompt: str) -> str:
        """
        –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∏–º–∏—Ç–∞—Ü–∏—è LLM (–≤ —Ä–µ–∞–ª—å–Ω–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã –≤—ã–∑–æ–≤ GPT-4 –∏–ª–∏ –ø–æ–¥–æ–±–Ω–æ–π –º–æ–¥–µ–ª–∏)
        """
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –∑–¥–µ—Å—å –±—ã–ª –±—ã –≤—ã–∑–æ–≤ –∫ API LLM
        return f"""–ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —è –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å.

–û—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö, –≤–æ—Ç —á—Ç–æ —è –º–æ–≥—É —Å–∫–∞–∑–∞—Ç—å:

–î–æ–∫—É–º–µ–Ω—Ç—ã –æ–ø–∏—Å—ã–≤–∞—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –≤–∫–ª—é—á–∞—è –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤. 
Jina Embeddings –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –∫–∞–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Ç–µ–∫—Å—Ç–∞ –≤ RAG —Å–∏—Å—Ç–µ–º–∞—Ö.

–≠—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∏–π —Ä–∞–±–æ—Ç—É RAG —Å–∏—Å—Ç–µ–º—ã. –í —Ä–µ–∞–ª—å–Ω–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç –æ—Ç –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–π LLM –º–æ–¥–µ–ª–∏."""

    def ask(self, question: str, top_k: int = 3, verbose: bool = True) -> str:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –∫ RAG —Å–∏—Å—Ç–µ–º–µ"""
        if verbose:
            print(f"üß† –í–æ–ø—Ä–æ—Å: {question}")
            print("üîç –ò—â—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã...")
        
        # –≠—Ç–∞–ø 1: Retrieval
        relevant_docs = self.retrieve(question, top_k)
        
        if verbose:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(relevant_docs)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
            for i, (doc, score, metadata) in enumerate(relevant_docs):
                print(f"   {i+1}. [–°—Ö–æ–¥—Å—Ç–≤–æ: {score:.3f}] {doc[:80]}...")
            print("ü§ñ –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç...")
        
        # –≠—Ç–∞–ø 2: Augmentation + Generation
        prompt = self.generate_prompt(question, relevant_docs)
        answer = self.mock_llm_generate(prompt)
        
        if verbose:
            print("=" * 80)
            print(f"üí¨ –û—Ç–≤–µ—Ç: {answer}")
            print("=" * 80)
        
        return {
            "question": question,
            "answer": answer,
            "relevant_documents": [
                {
                    "content": doc,
                    "similarity_score": float(score),
                    "metadata": metadata
                } for doc, score, metadata in relevant_docs
            ]
        }

def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã RAG —Å–∏—Å—Ç–µ–º—ã"""
    print("üöÄ –ó–∞–≥—Ä—É–∂–∞—é RAG —Å–∏—Å—Ç–µ–º—É —Å Jina Embeddings...")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã
    rag_system = RAGSystem()
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
    demo_questions = [
        "–ß—Ç–æ —Ç–∞–∫–æ–µ Jina Embeddings?",
        "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç RAG?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?",
        "–ö–∞–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —É –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞?"
    ]
    
    print("\n" + "=" * 60)
    print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø RAG –°–ò–°–¢–ï–ú–´")
    print("=" * 60)
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
    for i, question in enumerate(demo_questions, 1):
        print(f"\nüìã –ü—Ä–∏–º–µ—Ä {i}:")
        result = rag_system.ask(question)
        
        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        if i < len(demo_questions):
            print("\n" + "-" * 60)

if __name__ == "__main__":
    main()
