import torch
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    AutoProcessor,
    pipeline,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import Dataset, load_dataset
import pandas as pd
from typing import Dict, List

class QwenQAWithLoRA:
    def __init__(self, model_name: str = "Qwen/Qwen2.5-1.5B", use_4bit: bool = True):
        self.model_name = model_name
        self.setup_device()
        self.load_model_and_tokenizer(use_4bit)
        
    def setup_device(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
    
    def load_model_and_tokenizer(self, use_4bit: bool):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"""
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...")
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 4-bit quantization –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        if use_4bit:
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
            )
        else:
            quantization_config = None
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.float16
        )
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name, 
            trust_remote_code=True
        )
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ pad token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        print("‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
    
    def setup_lora(self, lora_rank: int = 8, lora_alpha: int = 16, lora_dropout: float = 0.05):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA –¥–ª—è —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"""
        print("üéØ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—é LoRA...")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è k-bit –æ–±—É—á–µ–Ω–∏—è
        self.model = prepare_model_for_kbit_training(self.model)
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LoRA
        lora_config = LoraConfig(
            r=lora_rank,
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            bias="none",
            task_type="CAUSAL_LM",
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]  # –ú–æ–¥—É–ª–∏ –¥–ª—è Qwen
        )
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ LoRA –∫ –º–æ–¥–µ–ª–∏
        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
        
        print("‚úÖ LoRA –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞!")
    
    def prepare_qa_dataset(self, questions: List[str], answers: List[str]) -> Dataset:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è QA"""
        print("üìö –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞—é –¥–∞—Ç–∞—Å–µ—Ç...")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
        formatted_data = []
        for question, answer in zip(questions, answers):
            prompt = f"""<|im_start|>system
–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã. –û—Ç–≤–µ—á–∞–π —Ç–æ—á–Ω–æ –∏ –ø–æ –¥–µ–ª—É.<|im_end|>
<|im_start|>user
{question}<|im_end|>
<|im_start|>assistant
{answer}<|im_end|>"""
            
            formatted_data.append({"text": prompt})
        
        return Dataset.from_list(formatted_data)
    
    def tokenize_function(self, examples):
        """–§—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        return self.tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=512,
            return_tensors="pt"
        )
    
    def train(self, questions: List[str], answers: List[str], training_args: Dict = None):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å LoRA"""
        print("üéì –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ...")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
        dataset = self.prepare_qa_dataset(questions, answers)
        tokenized_dataset = dataset.map(self.tokenize_function, batched=True)
        
        # –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        default_args = {
            "per_device_train_batch_size": 2,
            "gradient_accumulation_steps": 4,
            "warmup_steps": 20,
            "num_train_epochs": 3,
            "learning_rate": 2e-4,
            "fp16": True,
            "logging_steps": 10,
            "output_dir": "./qwen-qa-lora",
            "save_strategy": "epoch"
        }
        
        if training_args:
            default_args.update(training_args)
        
        from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling
        
        # –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è
        training_args = TrainingArguments(**default_args)
        
        # Data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
        )
        
        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
            tokenizer=self.tokenizer,
        )
        
        # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
        trainer.train()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        trainer.save_model()
        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∏ –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
    
    def answer_question(self, question: str, max_length: int = 200) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å"""
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
        prompt = f"""<|im_start|>system
–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã. –û—Ç–≤–µ—á–∞–π —Ç–æ—á–Ω–æ –∏ –ø–æ –¥–µ–ª—É.<|im_end|>
<|im_start|>user
{question}<|im_end|>
<|im_start|>assistant
"""
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=False)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        response = response.split("<|im_start|>assistant")[-1]
        response = response.split("<|im_end|>")[0].strip()
        
        return response

class MultimodalQwenQA:
    """–ö–ª–∞—Å—Å –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ QA (VLM)"""
    
    def __init__(self):
        self.setup_device()
    
    def setup_device(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
    
    def load_multimodal_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            from transformers import AutoModelForVision2Seq
            self.processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-7B", trust_remote_code=True)
            self.vl_model = AutoModelForVision2Seq.from_pretrained(
                "Qwen/Qwen2-VL-7B",
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            print("‚úÖ –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ VLM –º–æ–¥–µ–ª–∏: {e}")
            print("üí° –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install transformers torch pillow")
    
    def answer_with_image(self, image_path: str, question: str) -> str:
        """–û—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        from PIL import Image
        
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image = Image.open(image_path)
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": question}
                    ]
                }
            ]
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞
            text = self.processor.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            inputs = self.processor(
                text=text,
                images=image,
                padding=True,
                return_tensors="pt"
            ).to(self.device)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            with torch.no_grad():
                generated_ids = self.vl_model.generate(
                    **inputs,
                    max_new_tokens=100,
                    do_sample=False
                )
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
            generated_text = self.processor.batch_decode(
                generated_ids, 
                skip_special_tokens=True
            )[0]
            
            return generated_text
            
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}"

def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã QA —Å–∏—Å—Ç–µ–º—ã"""
    
    # –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    training_questions = [
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç?",
        "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏?",
        "–ö–∞–∫–∏–µ –≤–∏–¥—ã –æ–±—É—á–µ–Ω–∏—è –±—ã–≤–∞—é—Ç –≤ ML?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ?"
    ]
    
    training_answers = [
        "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç (–ò–ò) - —ç—Ç–æ –æ–±–ª–∞—Å—Ç—å –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –Ω–∞—É–∫, –∑–∞–Ω–∏–º–∞—é—â–∞—è—Å—è —Å–æ–∑–¥–∞–Ω–∏–µ–º –º–∞—à–∏–Ω, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏, —Ç—Ä–µ–±—É—é—â–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.",
        "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ –º–µ—Ç–æ–¥ –ò–ò, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ —è–≤–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, –Ω–∞—Ö–æ–¥—è patterns –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏.",
        "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ - —ç—Ç–æ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏ –º–æ–∑–≥–∞, —Å–æ—Å—Ç–æ—è—â–∏–µ –∏–∑ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∞–Ω–Ω—ã—Ö —É–∑–ª–æ–≤ (–Ω–µ–π—Ä–æ–Ω–æ–≤).",
        "–í –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –µ—Å—Ç—å supervised learning (—Å —É—á–∏—Ç–µ–ª–µ–º), unsupervised learning (–±–µ–∑ —É—á–∏—Ç–µ–ª—è) –∏ reinforcement learning (—Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º).",
        "–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ª–æ–∂–Ω—ã—Ö patterns –≤ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö."
    ]
    
    print("üöÄ –ó–∞–ø—É—Å–∫–∞—é QA —Å–∏—Å—Ç–µ–º—É —Å Qwen –∏ LoRA...")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è QA —Å–∏—Å—Ç–µ–º—ã
    qa_system = QwenQAWithLoRA(model_name="Qwen/Qwen2.5-1.5B", use_4bit=True)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA
    qa_system.setup_lora(lora_rank=8, lora_alpha=16)
    
    # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–µ–±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö (–º–æ–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –µ—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è)
    print("ü§ñ –û–±—É—á–∞—é –º–æ–¥–µ–ª—å –Ω–∞ QA –¥–∞–Ω–Ω—ã—Ö...")
    qa_system.train(training_questions, training_answers, {
        "per_device_train_batch_size": 1,
        "gradient_accumulation_steps": 4,
        "num_train_epochs": 2,
        "learning_rate": 1e-4
    })
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
    test_questions = [
        "–û–±—ä—è—Å–Ω–∏ —á—Ç–æ —Ç–∞–∫–æ–µ –ò–ò",
        "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–∏?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ supervised learning?"
    ]
    
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä—É—é —Å–∏—Å—Ç–µ–º—É:")
    print("=" * 50)
    
    for question in test_questions:
        answer = qa_system.answer_question(question)
        print(f"‚ùì –í–æ–ø—Ä–æ—Å: {question}")
        print(f"ü§ñ –û—Ç–≤–µ—Ç: {answer}")
        print("-" * 50)
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
    print("\nüëÅÔ∏è –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ QA (VLM):")
    multimodal_qa = MultimodalQwenQA()
    multimodal_qa.load_multimodal_model()
    
    # –ü—Ä–∏–º–µ—Ä —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º (–∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –µ—Å–ª–∏ –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
    # image_answer = multimodal_qa.answer_with_image("path/to/your/image.jpg", "–ß—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–æ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ?")
    # print(f"üñºÔ∏è –û—Ç–≤–µ—Ç –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é: {image_answer}")

if __name__ == "__main__":
    main()
