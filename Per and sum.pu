import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForSeq2SeqLM,
    AutoModelForCausalLM,
    pipeline
)
from sentence_transformers import SentenceTransformer, util
import numpy as np
from typing import List, Dict, Union
import re
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import warnings
warnings.filterwarnings('ignore')

@dataclass
class TextProcessingConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
    max_input_length: int = 1024
    max_output_length: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.1

class TextProcessor:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
    
    def __init__(self, model_type: str = "summarization"):
        self.config = TextProcessingConfig()
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
        self.load_models(model_type)
        
        # –ú–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å–º—ã—Å–ª–∞
        self.similarity_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
    
    def load_models(self, model_type: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π"""
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞...")
        
        if model_type == "summarization":
            # –ú–æ–¥–µ–ª—å –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            self.summarization_tokenizer = AutoTokenizer.from_pretrained("IlyaGusev/rut5_base_sum_gazeta")
            self.summarization_model = AutoModelForSeq2SeqLM.from_pretrained("IlyaGusev/rut5_base_sum_gazeta")
            
        elif model_type == "paraphrase":
            # –ú–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è
            self.paraphrase_tokenizer = AutoTokenizer.from_pretrained("sberbank-ai/rugpt3large_based_on_gpt2")
            self.paraphrase_model = AutoModelForCausalLM.from_pretrained("sberbank-ai/rugpt3large_based_on_gpt2")
        
        # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ–±–µ–∏—Ö –∑–∞–¥–∞—á
        self.universal_pipeline = pipeline(
            "text2text-generation",
            model="csebuetnlp/mT5_multilingual_XLSum",
            tokenizer="csebuetnlp/mT5_multilingual_XLSum",
            device=0 if self.device == "cuda" else -1
        )
        
        print("‚úÖ –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")

class AdvancedTextProcessor(TextProcessor):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏"""
    
    def __init__(self, model_type: str = "universal"):
        super().__init__(model_type)
        self.style_presets = {
            "formal": "–ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –≤ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–º –¥–µ–ª–æ–≤–æ–º —Å—Ç–∏–ª–µ",
            "casual": "–ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –≤ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–æ–º –Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω–æ–º —Å—Ç–∏–ª–µ", 
            "academic": "–ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –≤ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–º –Ω–∞—É—á–Ω–æ–º —Å—Ç–∏–ª–µ",
            "marketing": "–ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –≤ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–æ–º –ø—Ä–æ–¥–∞—é—â–µ–º —Å—Ç–∏–ª–µ",
            "technical": "–ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–º —Å—Ç–∏–ª–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏"
        }
        
        self.summary_types = {
            "brief": "–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –º—ã—Å–ª–µ–π",
            "detailed": "–°–æ–∑–¥–∞–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç–æ–µ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Å –∫–ª—é—á–µ–≤—ã–º–∏ –¥–µ—Ç–∞–ª—è–º–∏",
            "bullet_points": "–°–æ–∑–¥–∞–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –≤ –≤–∏–¥–µ –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞",
            "executive": "–°–æ–∑–¥–∞–π executive summary –¥–ª—è —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞"
        }

    def chunk_text(self, text: str, chunk_size: int = 1000) -> List[str]:
        """–†–∞–∑–±–∏–≤–∫–∞ –¥–ª–∏–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        sentences = re.split(r'(?<=[.!?])\s+', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk + sentence) < chunk_size:
                current_chunk += sentence + " "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + " "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
            
        return chunks

    def calculate_similarity(self, original: str, processed: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤"""
        emb_original = self.similarity_model.encode(original, convert_to_tensor=True)
        emb_processed = self.similarity_model.encode(processed, convert_to_tensor=True)
        
        similarity = util.pytorch_cos_sim(emb_original, emb_processed)
        return similarity.item()

    def paraphrase_text(self, 
                       text: str, 
                       style: str = "formal",
                       preserve_meaning: bool = True,
                       max_retries: int = 3) -> Dict[str, Union[str, float]]:
        """–ü—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–º—ã—Å–ª–∞"""
        
        prompt = f"""
        {self.style_presets.get(style, "–ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç")}:
        
        –û–†–ò–ì–ò–ù–ê–õ: {text}
        
        –ü–ï–†–ï–§–†–ê–ó–ò–†–û–í–ê–ù–ù–´–ô –¢–ï–ö–°–¢:
        """
        
        for attempt in range(max_retries):
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
                result = self.universal_pipeline(
                    prompt,
                    max_length=len(text.split()) + 50,
                    num_return_sequences=1,
                    temperature=self.config.temperature,
                    do_sample=True,
                    top_p=self.config.top_p,
                    repetition_penalty=self.config.repetition_penalty
                )
                
                paraphrased = result[0]['generated_text']
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–º—ã—Å–ª–∞
                if preserve_meaning:
                    similarity = self.calculate_similarity(text, paraphrased)
                    if similarity < 0.6:  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏
                        print(f"‚ö†Ô∏è –ù–∏–∑–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å ({similarity:.3f}), –ø—Ä–æ–±—É—é —Å–Ω–æ–≤–∞...")
                        continue
                
                return {
                    "original": text,
                    "paraphrased": paraphrased,
                    "similarity_score": similarity if preserve_meaning else None,
                    "style": style,
                    "success": True
                }
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏–∏ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                continue
        
        return {
            "original": text,
            "paraphrased": text,  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –≤ —Å–ª—É—á–∞–µ –Ω–µ—É–¥–∞—á–∏
            "similarity_score": 1.0,
            "style": style,
            "success": False
        }

    def summarize_text(self, 
                      text: str, 
                      summary_type: str = "brief",
                      compression_ratio: float = 0.3) -> Dict[str, Union[str, int]]:
        """–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏"""
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–µ–ª–µ–≤—É—é –¥–ª–∏–Ω—É
        target_length = max(50, int(len(text.split()) * compression_ratio))
        
        prompt = f"""
        {self.summary_types.get(summary_type, "–°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç")}:
        
        –¢–ï–ö–°–¢: {text}
        
        –°–£–ú–ú–ê–†–ò–ó–ê–¶–ò–Ø:
        """
        
        try:
            result = self.universal_pipeline(
                prompt,
                max_length=target_length + 100,
                min_length=max(30, target_length - 50),
                num_return_sequences=1,
                temperature=0.3,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                do_sample=True,
                top_p=0.8
            )
            
            summary = result[0]['generated_text']
            
            # –ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            summary = self.clean_summary(summary)
            
            return {
                "original": text,
                "summary": summary,
                "summary_type": summary_type,
                "compression_ratio": len(summary.split()) / len(text.split()),
                "original_length": len(text.split()),
                "summary_length": len(summary.split()),
                "success": True
            }
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return {
                "original": text,
                "summary": text[:500] + "..." if len(text) > 500 else text,
                "summary_type": summary_type,
                "compression_ratio": 1.0,
                "original_length": len(text.split()),
                "summary_length": len(text.split()),
                "success": False
            }

    def clean_summary(self, summary: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
        # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ñ—Ä–∞–∑—ã
        sentences = summary.split('. ')
        unique_sentences = []
        seen_sentences = set()
        
        for sentence in sentences:
            clean_sentence = sentence.strip()
            if clean_sentence and clean_sentence not in seen_sentences:
                unique_sentences.append(clean_sentence)
                seen_sentences.add(clean_sentence)
        
        return '. '.join(unique_sentences)

    def process_batch(self, 
                     texts: List[str], 
                     operation: str = "paraphrase",
                     **kwargs) -> List[Dict]:
        """–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
        
        with ThreadPoolExecutor(max_workers=4) as executor:
            if operation == "paraphrase":
                results = list(executor.map(
                    lambda text: self.paraphrase_text(text, **kwargs), 
                    texts
                ))
            elif operation == "summarize":
                results = list(executor.map(
                    lambda text: self.summarize_text(text, **kwargs), 
                    texts
                ))
            else:
                raise ValueError("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏: 'paraphrase', 'summarize'")
        
        return results

    def analyze_text_complexity(self, text: str) -> Dict[str, float]:
        """–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
        words = text.split()
        sentences = re.split(r'[.!?]+', text)
        
        # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        avg_sentence_length = len(words) / len(sentences) if sentences else 0
        
        # –°–ª–æ–∂–Ω–æ—Å—Ç—å —Å–ª–æ–≤ (–ø—Ä–æ—Ü–µ–Ω—Ç –¥–ª–∏–Ω–Ω—ã—Ö —Å–ª–æ–≤)
        long_words = [word for word in words if len(word) > 6]
        complexity_ratio = len(long_words) / len(words) if words else 0
        
        return {
            "word_count": len(words),
            "sentence_count": len(sentences),
            "avg_sentence_length": avg_sentence_length,
            "complexity_ratio": complexity_ratio,
            "readability_score": max(0, 100 - (avg_sentence_length + complexity_ratio * 100))
        }

class TextProcessingPipeline:
    """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
    
    def __init__(self):
        self.processor = AdvancedTextProcessor()
    
    def run_complete_pipeline(self, text: str) -> Dict:
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        
        print("üöÄ –ó–∞–ø—É—Å–∫–∞—é –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞...")
        
        # –ê–Ω–∞–ª–∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        complexity = self.processor.analyze_text_complexity(text)
        print(f"üìä –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: {complexity}")
        
        results = {}
        
        # –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
        print("üìù –í—ã–ø–æ–ª–Ω—è—é —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é...")
        summary_results = self.processor.summarize_text(
            text, 
            summary_type="executive",
            compression_ratio=0.3
        )
        results['summarization'] = summary_results
        
        # –ü—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ä–∞–∑–Ω—ã—Ö —Å—Ç–∏–ª—è—Ö
        print("üîÑ –í—ã–ø–æ–ª–Ω—è—é –ø—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏–µ...")
        paraphrase_styles = ["formal", "casual", "technical"]
        paraphrases = {}
        
        for style in paraphrase_styles:
            paraphrase_result = self.processor.paraphrase_text(
                text, 
                style=style,
                preserve_meaning=True
            )
            paraphrases[style] = paraphrase_result
        
        results['paraphrasing'] = paraphrases
        
        # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        results['analysis'] = {
            "original_complexity": complexity,
            "summary_compression": summary_results['compression_ratio'],
            "best_paraphrase_style": max(
                paraphrases.items(),
                key=lambda x: x[1]['similarity_score'] if x[1]['success'] else 0
            )[0] if any(p['success'] for p in paraphrases.values()) else "none"
        }
        
        return results

def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    
    # –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    sample_text = """
    –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–¥–Ω—É –∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö –∏ –±—ã—Å—Ç—Ä–æ—Ä–∞–∑–≤–∏–≤–∞—é—â–∏—Ö—Å—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç–∏. 
    –ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≥–æ–¥—ã –º—ã –Ω–∞–±–ª—é–¥–∞–µ–º –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. 
    –≠—Ç–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –Ω–∞—Ö–æ–¥—è—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ —Å–∞–º—ã—Ö —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ñ–µ—Ä–∞—Ö: –æ—Ç –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –¥–æ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞.
    
    –í–∞–∂–Ω—ã–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º —Ä–∞–∑–≤–∏—Ç–∏—è –ò–ò —è–≤–ª—è–µ—Ç—Å—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ GPT-4, 
    —Å–ø–æ—Å–æ–±–Ω—ã –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã, –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ –¥–∞–∂–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã–π –∫–æ–¥. –û–¥–Ω–∞–∫–æ –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –≤—Å–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è, 
    –ø–µ—Ä–µ–¥ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º–∏‰ªçÁÑ∂ —Å—Ç–æ—è—Ç —Å–µ—Ä—å–µ–∑–Ω—ã–µ –≤—ã–∑–æ–≤—ã, –≤–∫–ª—é—á–∞—è –ø—Ä–æ–±–ª–µ–º—É –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç–∏ –ò–ò –∏ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —ç—Ç–∏—á–Ω–æ—Å—Ç–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤.
    
    –í –±–ª–∏–∂–∞–π—à–∏–µ –≥–æ–¥—ã –æ–∂–∏–¥–∞–µ—Ç—Å—è –¥–∞–ª—å–Ω–µ–π—à–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –ò–ò –≤ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—É—é –∂–∏–∑–Ω—å. –ö–æ–º–ø–∞–Ω–∏–∏ –≤—Å–µ –∞–∫—Ç–∏–≤–Ω–µ–µ –≤–Ω–µ–¥—Ä—è—é—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã 
    –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –±–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏ —É–ª—É—á—à–µ–Ω–∏—è –∫–ª–∏–µ–Ω—Ç—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞. –ü—Ä–∏ —ç—Ç–æ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–º –æ—Å—Ç–∞–µ—Ç—Å—è –≤–æ–ø—Ä–æ—Å –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö 
    —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤, —Å–ø–æ—Å–æ–±–Ω—ã—Ö —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å —Ç–∞–∫–∏–µ —Å–∏—Å—Ç–µ–º—ã.
    """
    
    print("=" * 80)
    print("üéØ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ü–ê–ô–ü–õ–ê–ô–ù–ê –ü–†–ï–§–†–ê–ó–ò–†–û–í–ê–ù–ò–Ø –ò –°–£–ú–ú–ê–†–ò–ó–ê–¶–ò–ò")
    print("=" * 80)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞
    pipeline = TextProcessingPipeline()
    
    # –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    results = pipeline.run_complete_pipeline(sample_text)
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n" + "=" * 80)
    print("üìã –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–†–ê–ë–û–¢–ö–ò")
    print("=" * 80)
    
    # –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
    print("\nüìÑ –°–£–ú–ú–ê–†–ò–ó–ê–¶–ò–Ø:")
    print(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {results['summarization']['original_length']} —Å–ª–æ–≤")
    print(f"–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è: {results['summarization']['summary_length']} —Å–ª–æ–≤")
    print(f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è: {results['summarization']['compression_ratio']:.2f}")
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {results['summarization']['summary']}")
    
    # –ü—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
    print("\nüîÑ –ü–†–ï–§–†–ê–ó–ò–†–û–í–ê–ù–ò–ï:")
    for style, result in results['paraphrasing'].items():
        if result['success']:
            print(f"\nüé≠ –°—Ç–∏–ª—å: {style.upper()}")
            print(f"–°—Ö–æ–∂–µ—Å—Ç—å: {result['similarity_score']:.3f}")
            print(f"–¢–µ–∫—Å—Ç: {result['paraphrased']}")
    
    # –ê–Ω–∞–ª–∏–∑
    print("\nüìä –ê–ù–ê–õ–ò–¢–ò–ö–ê:")
    print(f"–õ—É—á—à–∏–π —Å—Ç–∏–ª—å –¥–ª—è –ø—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è: {results['analysis']['best_paraphrase_style']}")
    print(f"–°–ª–æ–∂–Ω–æ—Å—Ç—å –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {results['analysis']['original_complexity']['readability_score']:.1f}/100")

if __name__ == "__main__":
    main()
